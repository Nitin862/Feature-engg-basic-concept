{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1054f4d9-21bb-4464-ae3f-c99612c68107",
   "metadata": {},
   "source": [
    "Q1. What is the Filter method in feature selection, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c339c8c-633c-433b-9bed-22596797e88e",
   "metadata": {},
   "source": [
    "\n",
    "In feature selection, the Filter method is a technique used to select the most relevant features\n",
    "from a dataset based on their statistical properties. It operates independently of any specific machine learning algorithm. Here's how it works:\n",
    "\n",
    "Feature Ranking: In the first step, each feature's relevance is assessed individually without considering the interaction with other features.\n",
    "Various statistical measures such as correlation coefficient, chi-square statistic, mutual information, or ANOVA F-value are commonly used for\n",
    "ranking features.\n",
    "\n",
    "Selection Criterion: Once the features are ranked, a selection criterion is applied to determine which features to keep and which to discard. \n",
    "The criterion could be a fixed threshold value, selecting the top N features, or using a statistical test to identify significant features.\n",
    "\n",
    "Independence from the Learning Algorithm: Unlike wrapper methods, which involve training a model to evaluate feature subsets, the Filter method\n",
    "evaluates features based solely on their intrinsic properties, independent of any particular learning algorithm. This makes it computationally less\n",
    "expensive and faster, especially for high-dimensional datasets.\n",
    "\n",
    "Preprocessing: Before applying the Filter method, it's essential to preprocess the data appropriately, such as handling missing values, encoding \n",
    "categorical variables, and scaling numerical features, to ensure the statistical measures used for feature ranking are meaningful.\n",
    "\n",
    "Evaluation: Finally, the selected subset of features is evaluated using a machine learning algorithm to assess its performance. While the Filter\n",
    "method is efficient for feature selection, it may not always yield the optimal subset of features for a specific learning task. Therefore, \n",
    "it's often used in conjunction with other feature selection techniques like Wrapper and Embedded methods for better performance.\n",
    "\n",
    "Overall, the Filter method offers a simple and computationally efficient approach to feature selection by leveraging statistical properties of \n",
    "individual features, making it suitable for preprocessing steps in the machine learning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f3800c-27da-4535-8fe9-45dc77c0aacf",
   "metadata": {},
   "source": [
    "Q2. How does the Wrapper method differ from the Filter method in feature selection?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be7329-d1a1-4723-af34-497a3aa12654",
   "metadata": {},
   "source": [
    "The Wrapper method differs from the Filter method in feature selection in the following ways:\n",
    "\n",
    "Evaluation Strategy:\n",
    "\n",
    "Wrapper Method: In the Wrapper method, feature subsets are evaluated based on their performance with a specific machine learning algorithm.\n",
    "It involves training and testing multiple models with different feature subsets to identify the best-performing subset.\n",
    "Filter Method: In contrast, the Filter method evaluates features based on their intrinsic properties, such as statistical measures like \n",
    "correlation or information gain, independent of any specific machine learning algorithm.\n",
    "Computational Cost:\n",
    "\n",
    "Wrapper Method: Because the Wrapper method involves training and evaluating multiple models with different feature subsets,\n",
    "it can be computationally expensive, especially for high-dimensional datasets or complex machine learning algorithms.\n",
    "Filter Method: The Filter method, on the other hand, is computationally less expensive since it doesn't involve training and \n",
    "testing machine learning models. Instead, it relies on statistical properties of features for selection.\n",
    "Dependency on Learning Algorithm:\n",
    "\n",
    "Wrapper Method: The performance of feature subsets in the Wrapper method is highly dependent on the choice of the machine learning algorithm \n",
    "used for evaluation. Different algorithms may lead to different subsets being selected as optimal.\n",
    "Filter Method: In contrast, the Filter method is independent of any specific learning algorithm. It evaluates features based on their intrinsic\n",
    "properties, making it more generalizable across different learning tasks.\n",
    "Optimization Goal:\n",
    "\n",
    "Wrapper Method: The goal of the Wrapper method is to find the optimal subset of features that maximizes the performance of a specific machine\n",
    "learning algorithm on the task at hand. It considers the interaction between features and their impact on model performance.\n",
    "Filter Method: The Filter method aims to select features based on their individual properties, such as relevance or information content, without\n",
    "considering their interaction with the learning algorithm. It focuses on reducing the dimensionality of the dataset while preserving relevant\n",
    "information.\n",
    "Risk of Overfitting:\n",
    "\n",
    "Wrapper Method: Because the Wrapper method directly optimizes the performance of a specific machine learning algorithm on the training data, \n",
    "there's a risk of overfitting to the training set, especially if the evaluation is not performed carefully.\n",
    "Filter Method: The Filter method, being based on intrinsic properties of features, is less prone to overfitting since it doesn't involve training\n",
    "models directly on the dataset.\n",
    "In summary, while both Wrapper and Filter methods are used for feature selection, they differ in their evaluation strategy, computational cost,\n",
    "dependency on learning algorithm, optimization goal, and risk of overfitting. The choice between the two methods depends on the specific characteristics of the dataset, computational resources available, and the goals of the feature selection process.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1338a19-93bc-4403-981c-5e2572866777",
   "metadata": {},
   "source": [
    "Q3. What are some common techniques used in Embedded feature selection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225a927c-edfb-4447-87c5-857c4adbd62f",
   "metadata": {},
   "source": [
    "Embedded feature selection methods integrate feature selection within the process of training a machine learning algorithm.\n",
    "Here are some common techniques used in Embedded feature selection methods:\n",
    "\n",
    "Lasso Regression (L1 Regularization):\n",
    "\n",
    "Lasso regression adds a penalty term (L1 regularization) to the ordinary least squares objective function, encouraging sparse feature \n",
    "coefficients. As a result, some coefficients are driven to zero, effectively performing feature selection.\n",
    "Ridge Regression (L2 Regularization):\n",
    "\n",
    "Ridge regression adds a penalty term (L2 regularization) to the ordinary least squares objective function, which penalizes large coefficients.\n",
    "While it doesn't perform feature selection directly, it can still shrink less relevant features, effectively reducing their impact on the model.\n",
    "Elastic Net Regression:\n",
    "\n",
    "Elastic Net combines L1 and L2 regularization penalties. It balances between the L1 and L2 penalties, providing a compromise between variable \n",
    "selection (like Lasso) and coefficient shrinkage (like Ridge).\n",
    "Decision Trees (e.g., Random Forest, Gradient Boosting):\n",
    "\n",
    "Decision tree-based algorithms inherently perform feature selection during the training process. Features that contribute the most to reducing\n",
    "impurity (e.g., Gini impurity, entropy) are selected for splitting nodes. Random Forest and Gradient Boosting models can further rank features\n",
    "based on their importance scores, providing a built-in feature selection mechanism.\n",
    "Feature Importance from Ensemble Methods:\n",
    "\n",
    "Ensemble methods like Random Forest, Gradient Boosting, and AdaBoost can provide feature importance scores based on how frequently features are\n",
    "used across multiple models or iterations. These importance scores can be used for feature selection by retaining only the most important features.\n",
    "Regularized Linear Models:\n",
    "\n",
    "Regularized linear models like Logistic Regression with L1 or L2 regularization can perform feature selection by penalizing coefficients associated \n",
    "with less relevant features, similar to Lasso or Ridge regression.\n",
    "Neural Network Pruning:\n",
    "\n",
    "In deep learning, techniques such as weight pruning or neuron pruning can be applied during or after training to remove less important connections\n",
    "or neurons, effectively performing feature selection.\n",
    "Genetic Algorithms:\n",
    "\n",
    "Genetic algorithms can be used to evolve a population of feature subsets over multiple generations, optimizing towards a specific objective \n",
    "function, such as model accuracy or fitness score.\n",
    "These Embedded feature selection methods are advantageous because they incorporate feature selection directly into the model training process, \n",
    "potentially leading to better generalization performance and more efficient use of computational resources compared to separate feature selection techniques.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d4f044-3618-4832-91a3-dc541a960e13",
   "metadata": {},
   "source": [
    "Q4. What are some drawbacks of using the Filter method for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8eca68-fd74-4d87-81e9-051868bc0bb9",
   "metadata": {},
   "source": [
    "While the Filter method for feature selection has several advantages, it also comes with certain drawbacks:\n",
    "\n",
    "Independence Assumption:\n",
    "\n",
    "The Filter method evaluates features independently of each other based on their intrinsic properties such as correlation, \n",
    "mutual information, or statistical tests. However, this assumption may not always hold true in real-world datasets where features might have\n",
    "complex interactions or dependencies. Consequently, important features may be disregarded if their relevance is not adequately captured by \n",
    "individual feature metrics.\n",
    "Limited Consideration of Model Performance:\n",
    "\n",
    "Unlike Wrapper methods, which evaluate feature subsets based on actual model performance, the Filter method does not directly\n",
    "consider how selected features impact model performance. Therefore, it may not always lead to the selection of the most optimal feature subset\n",
    "for a specific machine learning task. Features selected solely based on statistical properties may not necessarily improve model performance.\n",
    "Inability to Capture Non-linear Relationships:\n",
    "\n",
    "Many filter methods rely on linear statistical measures such as correlation or mutual information, which might not effectively capture non-linear\n",
    "relationships between features and the target variable. Consequently, important features with non-linear relationships may be overlooked by filter\n",
    "methods.\n",
    "Sensitivity to Feature Scaling and Data Distribution:\n",
    "\n",
    "Filter methods often depend on statistical measures that can be sensitive to the scale and distribution of features. Therefore, preprocessing steps\n",
    "such as feature scaling and normalization are crucial to ensure meaningful feature selection results. Inconsistent scaling or distribution across \n",
    "features may lead to biased feature selection outcomes.\n",
    "Limited Exploration of Feature Subsets:\n",
    "\n",
    "Filter methods typically evaluate features individually and select a subset based on predefined criteria or thresholds. This approach may overlook \n",
    "synergistic effects or interactions among features that could contribute to improved model performance. Consequently, the selected feature subset may\n",
    "not fully exploit the potential predictive power of the dataset.\n",
    "Difficulty Handling Redundant Features:\n",
    "\n",
    "Filter methods may struggle to handle redundant features, i.e., features that convey similar information. Redundant features can inflate the\n",
    "importance of certain features or bias the feature selection process, leading to suboptimal results. Additional preprocessing steps or more advanced\n",
    "feature selection techniques may be required to address redundancy effectively.\n",
    "Overall, while the Filter method offers simplicity and computational efficiency in feature selection, it is important to be mindful of its limitations\n",
    "and potential drawbacks, especially in scenarios where complex feature interactions, non-linear relationships, or model performance optimization are\n",
    "critical considerations. Integrating multiple feature selection techniques or using hybrid approaches may help mitigate these limitations and improve\n",
    "feature selection outcomes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6008d1-c335-496f-af28-2cf65ae64587",
   "metadata": {},
   "source": [
    "Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature\n",
    "selection?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcbbf47-deb9-4d77-a31a-d3e936cb8dd3",
   "metadata": {},
   "source": [
    "The decision to use the Filter method over the Wrapper method for feature selection depends on various factors, including the characteristics of the dataset, computational resources, and the specific goals of the feature selection process. Here are some situations where the Filter method may be preferred:\n",
    "\n",
    "Large Datasets:\n",
    "\n",
    "Filter methods are computationally efficient and can handle large datasets with high dimensionality more effectively compared to Wrapper methods.\n",
    "When computational resources are limited or the dataset size is substantial, the Filter method can be a more practical choice.\n",
    "Preprocessing and Exploratory Analysis:\n",
    "\n",
    "Filter methods are often used as part of the preprocessing and exploratory analysis stage in the machine learning pipeline. They provide \n",
    "quick insights into feature relevance and can help identify potentially important features before proceeding to more computationally expensive \n",
    "Wrapper methods or model training.\n",
    "Initial Feature Screening:\n",
    "\n",
    "When dealing with a large pool of candidate features, the Filter method can serve as an initial screening mechanism to identify the most promising\n",
    "features for further evaluation. It helps reduce the search space and focus computational resources on a subset of potentially relevant features.\n",
    "Stability and Reproducibility:\n",
    "\n",
    "Filter methods typically yield stable and reproducible feature selection results since they are based on intrinsic properties of features rather \n",
    "than dependent on specific machine learning algorithms or training data splits. This stability can be advantageous, especially in scenarios where \n",
    "consistency in feature selection outcomes is important.\n",
    "Interpretability:\n",
    "\n",
    "Filter methods often provide straightforward and interpretable metrics for feature ranking and selection, such as correlation coefficients, mutual\n",
    "information, or statistical tests. These metrics offer insights into the relationship between individual features and the target variable, enhancing \n",
    "interpretability of the feature selection process.\n",
    "Noise Handling:\n",
    "\n",
    "In datasets where noisy features are present, Filter methods may offer better resilience compared to Wrapper methods. By relying on statistical\n",
    "properties of features rather than model performance, Filter methods can potentially filter out noisy features more effectively.\n",
    "Low Sample Size:\n",
    "\n",
    "In situations where the sample size is small, Wrapper methods may suffer from overfitting due to the limited amount of data available for training \n",
    "and evaluating multiple models. In such cases, the Filter method, which does not involve training models, may be more suitable to avoid overfitting.\n",
    "In summary, the Filter method is preferred over the Wrapper method in scenarios where computational efficiency, scalability, stability,\n",
    "interpretability, and robustness to noise are prioritized, especially during initial data exploration and preprocessing stages. However, \n",
    "it's important to assess the trade-offs and limitations of the Filter method in specific contexts and consider complementary approaches for comprehensive feature selection.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ea19ab-e3fa-442c-bc0c-498fdcf9a753",
   "metadata": {},
   "source": [
    "Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn.\n",
    "You are unsure of which features to include in the model because the dataset contains several different\n",
    "ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e90c65-dd02-49bb-9192-b28c2eaf422b",
   "metadata": {},
   "source": [
    "\n",
    "To choose the most pertinent attributes for the predictive model of customer churn using the Filter Method, you can follow these steps:\n",
    "\n",
    "Data Understanding and Preprocessing:\n",
    "\n",
    "Start by thoroughly understanding the dataset, including the meaning and nature of each feature. Identify any missing values,\n",
    "outliers, or inconsistencies in the data and preprocess it accordingly. Handle categorical variables by encoding them appropriately \n",
    "and scale numerical features if needed.\n",
    "Feature Ranking:\n",
    "\n",
    "Apply various statistical measures to rank the features based on their relevance to predicting customer churn. Common measures include:\n",
    "Correlation coefficient: Calculate the correlation between each feature and the target variable (churn). Features with higher absolute correlation\n",
    "values are considered more relevant.\n",
    "Mutual information: Measure the amount of information shared between each feature and the target variable. Features with higher mutual \n",
    "information are deemed more informative for predicting churn.\n",
    "Statistical tests (e.g., t-test, ANOVA): Evaluate the significance of the relationship between each feature and churn using appropriate \n",
    "statistical tests.\n",
    "Selecting Features:\n",
    "\n",
    "Set a threshold or criteria for feature selection based on the ranking obtained from the statistical measures. You can choose to keep features\n",
    "with correlation coefficients above a certain threshold, mutual information scores above a threshold, or p-values below a significance level\n",
    "for statistical tests.\n",
    "Alternatively, you can select the top N features based on their ranking scores.\n",
    "Validate Selection:\n",
    "\n",
    "Validate the selected features by assessing their impact on model performance using cross-validation or a holdout validation set. \n",
    "Train predictive models (e.g., logistic regression, decision trees, random forest) using the selected features and evaluate their\n",
    "performance metrics such as accuracy, precision, recall, and F1-score.\n",
    "Compare the performance of models trained with the selected features against models trained with all features or other feature selection\n",
    "methods to ensure the effectiveness of the Filter Method in improving model performance.\n",
    "Iterative Refinement:\n",
    "\n",
    "Iterate the process by experimenting with different thresholds or criteria for feature selection and evaluating the resulting model performance. \n",
    "Fine-tune the selection criteria based on the observed performance to achieve the best balance between predictive power and model simplicity.\n",
    "Interpret Results:\n",
    "\n",
    "Analyze the selected features and their relationship with customer churn to gain insights into factors influencing churn behavior. Interpret\n",
    "the results to understand the driving factors behind customer attrition and identify potential areas for intervention or improvement.\n",
    "By following these steps, you can effectively use the Filter Method to choose the most pertinent attributes for predicting customer churn \n",
    "in the telecom company's dataset. It provides a systematic approach to feature selection based on the statistical properties of the features, facilitating the development of an accurate and interpretable predictive model.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0763736b-5962-4f07-98e7-8d86f54c0674",
   "metadata": {},
   "source": [
    "Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with\n",
    "many features, including player statistics and team rankings. Explain how you would use the Embedded\n",
    "method to select the most relevant features for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc3473b-c5dd-45c0-a50e-4aca90258b35",
   "metadata": {},
   "source": [
    "To use the Embedded method for selecting the most relevant features for predicting the outcome of soccer matches, you can leverage machine \n",
    "learning algorithms that inherently perform feature selection during the training process. Here's how you can proceed:\n",
    "\n",
    "Data Understanding and Preprocessing:\n",
    "\n",
    "Begin by thoroughly understanding the dataset, including the available features such as player statistics, team rankings,\n",
    "match history, and other relevant information. Preprocess the data by handling missing values, encoding categorical variables,\n",
    "and scaling numerical features as necessary.\n",
    "Choose Embedded Algorithms:\n",
    "\n",
    "Select machine learning algorithms that support feature selection as part of their training process. Common choices include:\n",
    "Regularized Linear Models: Algorithms like Lasso Regression (L1 regularization) and Ridge Regression (L2 regularization) penalize \n",
    "coefficients associated with less relevant features, effectively performing feature selection.\n",
    "Tree-Based Models: Decision trees and ensemble methods such as Random Forest and Gradient Boosting inherently perform feature selection\n",
    "by selecting features for splitting nodes based on their importance in reducing impurity or error.\n",
    "Train Models:\n",
    "\n",
    "Train the selected machine learning algorithms using the entire dataset, including all available features. During the training process,\n",
    "these algorithms will automatically assess the importance of each feature and adjust their coefficients or feature importance scores accordingly.\n",
    "Feature Importance:\n",
    "\n",
    "Extract feature importance scores from the trained models. For regularized linear models, examine the coefficients associated with each feature.\n",
    "Features with non-zero coefficients in Lasso Regression or relatively large coefficients in Ridge Regression are considered more relevant.\n",
    "For tree-based models, such as Random Forest or Gradient Boosting, utilize the feature importance attribute provided by these models. Features \n",
    "with higher importance scores are deemed more informative for predicting the outcome of soccer matches.\n",
    "Feature Selection:\n",
    "\n",
    "Rank the features based on their importance scores obtained from the trained models. You can choose to keep features with the highest importance\n",
    "scores or set a threshold to select the top N features.\n",
    "Alternatively, you can perform iterative feature selection by recursively eliminating less important features based on their importance scores \n",
    "until reaching the desired number of features or a specified performance threshold.\n",
    "Validate Selection:\n",
    "\n",
    "Validate the selected features by training predictive models using only the selected features and evaluating their performance metrics on a validation set or through cross-validation. Assess the model's accuracy, precision, recall, and other relevant metrics to ensure the effectiveness of the selected features in predicting soccer match outcomes.\n",
    "By following these steps, you can effectively use the Embedded method to select the most relevant features for predicting the outcome of soccer\n",
    "matches. The advantage of this approach is that it integrates feature selection seamlessly into the model training process, resulting in a more efficient and interpretable predictive model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc89534e-c5b7-40a7-826e-f7786ee0f13e",
   "metadata": {},
   "source": [
    "Q8. You are working on a project to predict the price of a house based on its features, such as size, location,\n",
    "and age. You have a limited number of features, and you want to ensure that you select the most important\n",
    "ones for the model. Explain how you would use the Wrapper method to select the best set of features for the\n",
    "predictor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f344508d-7b33-415c-bdf5-b4078b0dd77c",
   "metadata": {},
   "source": [
    "To use the Wrapper method for selecting the best set of features for predicting the price of a house, you can follow these steps:\n",
    "\n",
    "Data Understanding and Preprocessing:\n",
    "\n",
    "Begin by thoroughly understanding the dataset, including the available features such as house size, location, age, and other relevant attributes. \n",
    "Preprocess the data by handling missing values, encoding categorical variables, and scaling numerical features as necessary.\n",
    "Select a Subset of Features:\n",
    "\n",
    "Choose a subset of features from the dataset to be evaluated for inclusion in the predictive model. Since you have a limited number of features,\n",
    "you can start with all available features as the initial subset.\n",
    "Define a Performance Metric:\n",
    "\n",
    "Define a performance metric to evaluate the quality of different feature subsets. For predicting house prices, common metrics include Mean Absolute\n",
    "Error (MAE), Mean Squared Error (MSE), or R-squared (R2) score. Choose a metric that aligns with the project objectives and the desired accuracy of the predictive model.\n",
    "Select a Model:\n",
    "\n",
    "Choose a machine learning model that can be trained using the selected subset of features. Regression models such as Linear Regression, Ridge\n",
    "Regression, Lasso Regression, or Decision Trees are commonly used for predicting house prices. Select a model that is suitable for the dataset and the\n",
    "chosen performance metric.\n",
    "Feature Subset Evaluation:\n",
    "\n",
    "Implement a feature selection algorithm that systematically evaluates different subsets of features using the chosen model and performance metric.\n",
    "Popular algorithms include:\n",
    "Forward Selection: Start with an empty set of features and iteratively add one feature at a time, selecting the feature that maximizes the improvement\n",
    "in the performance metric.\n",
    "Backward Elimination: Start with all features and iteratively remove one feature at a time, selecting the feature subset that maximizes the \n",
    "performance\n",
    "metric.\n",
    "Recursive Feature Elimination (RFE): Train the model on all features and recursively eliminate the least important features until reaching the \n",
    "desired number of features or optimal performance.\n",
    "Cross-Validation:\n",
    "\n",
    "Perform cross-validation to assess the generalization performance of each evaluated feature subset. Split the dataset into training and validation\n",
    "sets, train the model on the training set, and evaluate its performance on the validation set. Repeat this process multiple times with different\n",
    "training/validation splits to obtain robust performance estimates.\n",
    "Select the Best Feature Subset:\n",
    "\n",
    "Choose the feature subset that yields the best performance metric (e.g., lowest MAE, MSE, or highest R2 score) across the cross-validation folds. \n",
    "This subset represents the optimal set of features for predicting house prices based on the Wrapper method.\n",
    "Model Training and Evaluation:\n",
    "\n",
    "Train the final predictive model using the selected feature subset on the entire dataset. Evaluate the model's performance on a separate test\n",
    "set to assess its generalization ability and ensure that it performs well on unseen data.\n",
    "By following these steps, you can effectively use the Wrapper method to select the best set of features for predicting house prices based on the\n",
    "available dataset and performance objectives.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d831c1-7cee-485d-9689-c349a62d1d85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
